{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative models have been a popular topic in unsupervised learning. In recent three years, Variational Autoencoder (VAE) has been viewed as one of the most promising generative models for various type of data. In our project, we implemented VAE based on tensorflow; optimized the code with with parellelism, numpy, optimized numba and cython, and compare their performances; adding parellel; compared VAE with traditional autoencoder and Generative Adversarial Network (GAN).   \n",
    "Key word: Variational Autoencoder, generative models, parellel computing, code optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project is based on the paper [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) and a tutorial [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908.pdf). The paper provides the theoretical ground of a new inference method: vairational inference, which can used stochastic gradient descent as optimization method, and the possible applications on working as a generative model when combining neural network. The tutorial discussed the details when implementing the model.  \n",
    "Generative models are aiming at finding the distribution of data P(X) and sampling from that distribution, so that the intrinsic attributes for the data could be explored, and potentially have several applications like data augmentation, dialogue generation and machine translation. Traditional generative models either requires strong assumptions on data distribution or are expensive computationally. Recent emerged variational autoencoder (VAE) have weak assumptions and fast optimation method. Although VAE do make approximation, but the approximation error is arguably small given high-capacity models. Which makes VAE one of the most popular framework in recent years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To introduce the algorithm of Variational Autoencoder (VAE), we'd like to first introduct a little about traditional autoencoder. Autoencoder is a neural network framework that first encode the raw data into a representation (usually have lower dimension than raw data) z, then decode the representation to reconstructed data, so that the reconstructed data could be as similar with raw data as possible. Autoencoder is a useful method in feature dimension reduction, and also could be used in generative model. In VAE, we not only learn the encode and decode process, but also learn the proper distribution of the latent variables z (the encoded representation in autoencoder).  \n",
    "The standard VAE model could be represented as a graphical model shown below. \n",
    "![alt text](./img/VAE.png \"VAE.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lack of \"encoder\" process indicates that it is not necessary to have input data to complete the sample process. Here, z is the latent variable sampled from normal distribution. Through a trained $P(X|Z)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
